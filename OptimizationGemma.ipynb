{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-25T16:36:59.920544Z","iopub.status.busy":"2024-03-25T16:36:59.920200Z","iopub.status.idle":"2024-03-25T16:37:00.177996Z","shell.execute_reply":"2024-03-25T16:37:00.176686Z","shell.execute_reply.started":"2024-03-25T16:36:59.920515Z"},"trusted":true},"outputs":[],"source":["from kaggle_secrets import UserSecretsClient\n","user_secrets = UserSecretsClient()\n","secret_value_0 = user_secrets.get_secret(\"HF_TOKEN_READ\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:37:00.180439Z","iopub.status.busy":"2024-03-25T16:37:00.179777Z","iopub.status.idle":"2024-03-25T16:38:46.060122Z","shell.execute_reply":"2024-03-25T16:38:46.058928Z","shell.execute_reply.started":"2024-03-25T16:37:00.180404Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cubinlinker, which is not installed.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 23.8.0 requires ptxcompiler, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\n","apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.5 which is incompatible.\n","cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\n","s3fs 2024.3.0 requires fsspec==2024.3.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip3 install -q -U bitsandbytes==0.42.0\n","!pip3 install -q -U peft==0.8.2\n","!pip3 install -q -U trl==0.7.10\n","!pip3 install -q -U accelerate==0.27.1\n","!pip3 install -q -U datasets==2.17.0\n","!pip3 install -q -U transformers==4.38.0\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:38:46.062007Z","iopub.status.busy":"2024-03-25T16:38:46.061687Z","iopub.status.idle":"2024-03-25T16:38:53.934110Z","shell.execute_reply":"2024-03-25T16:38:53.933319Z","shell.execute_reply.started":"2024-03-25T16:38:46.061979Z"},"trusted":true},"outputs":[],"source":["\n","from peft import LoraConfig\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","from datasets import Dataset, DatasetDict\n","import json\n","\n","def load_custom_jsonl_to_dataset(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [json.loads(line.strip()) for line in file]\n","    dataset = Dataset.from_dict({\"question\": [item[\"question\"] for item in data],\n","                                 \"answer\": [item[\"answer\"] for item in data]})\n","    return DatasetDict({\"train\": dataset})\n","jsonl_file_path = \"/kaggle/input/halfdataset/HalfDataset.jsonl\"\n","custom_dataset = load_custom_jsonl_to_dataset(jsonl_file_path)\n","\n","def formatting_func(example):\n","    text = f\"<start_of_turn>user\\n{example['question'][0]}<end_of_turn> <start_of_turn>model\\n{example['answer'][0]}<end_of_turn>\"\n","    return [text]\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:38:53.935873Z","iopub.status.busy":"2024-03-25T16:38:53.935526Z","iopub.status.idle":"2024-03-25T16:56:12.458622Z","shell.execute_reply":"2024-03-25T16:56:12.457438Z","shell.execute_reply.started":"2024-03-25T16:38:53.935841Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-25 16:38:56.843186: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-03-25 16:38:56.843293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-03-25 16:38:56.983207: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cbe60d8d147471a8daf667bb9d0e40e","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dbdd0d2e89445cb992ec3a1c1fc356a","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6087c0929ea746bb9f07fa4d01920d94","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca449817ed6346f39bd070c1ac5dcfd8","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c4f9441a82dd4d05afb2f18be3a092cf","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01aa7408f4b349939c7232a28a629e07","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7d07e4c3b566441d865387bf31c23e79","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61084bb39dc34f65ada09cd332749e46","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9fa5c8fec1c49b3af876f7f29d5ed80","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b37cb923ece741fb9f44e2f29b3ff548","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"04322d82bd534993b5a07161c9effdfb","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"253683bf5e0047108d8d96fe7e8d58a7","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/13133 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240325_163940-kduhvjoo</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/sahils/huggingface/runs/kduhvjoo' target=\"_blank\">warm-frog-7</a></strong> to <a href='https://wandb.ai/sahils/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/sahils/huggingface' target=\"_blank\">https://wandb.ai/sahils/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/sahils/huggingface/runs/kduhvjoo' target=\"_blank\">https://wandb.ai/sahils/huggingface/runs/kduhvjoo</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [175/175 05:12, Epoch 50/59]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.005100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>6.882600</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>5.344000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5.916400</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>5.488100</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>5.837700</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>4.198200</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>3.900600</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.294100</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>3.468000</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>4.421300</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.984700</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>2.657200</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>3.017800</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.634300</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.453200</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>2.900300</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.057800</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>2.605400</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.883700</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.798800</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.797300</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.546200</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.526600</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.594500</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.551900</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.075100</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.993700</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.936000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.068500</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.825000</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.830300</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.704000</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.599000</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.538500</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.597900</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.478000</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.430100</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.364500</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.320800</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.354400</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.426400</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.288700</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.296200</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.425600</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.262600</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.309800</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.326600</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.210500</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.233500</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.238100</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.285500</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.296900</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.234900</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.206900</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.254300</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.289800</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.188200</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.203300</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.185200</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.182800</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.194000</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.282400</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.174100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.192000</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.195200</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.275300</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.152900</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.189200</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.218700</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.159500</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.191000</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.223100</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.158700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.153200</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.222500</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.185200</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.203400</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.219300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.129700</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.152500</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.204300</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.131700</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.163800</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.119900</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.175200</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.183800</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.142300</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.150600</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.170600</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.172700</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.154700</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.173700</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.145400</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.111400</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.186200</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.152000</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.114400</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.111300</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.126800</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.161700</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.194900</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.122600</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.115700</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.137200</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.179600</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>0.120700</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>0.108100</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.107400</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.108200</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.146700</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>0.131400</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>0.117600</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.144100</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.124500</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.131700</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.114700</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.086000</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.121400</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.111500</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.127100</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.109300</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.123400</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.136300</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.098700</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.083200</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.105500</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.112600</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.087800</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.105300</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.105100</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.099600</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.132000</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.081400</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.124900</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.111000</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.092300</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.081300</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.144600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.081400</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.086300</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>0.091100</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.131700</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.103000</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.084600</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.100800</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.095900</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.114700</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.096700</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.089100</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.103400</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.078700</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.109800</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.084400</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.109300</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.077700</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.116300</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.073500</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.085500</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.079500</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.122700</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.081800</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.114500</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.074100</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.097000</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.075800</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.096300</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.117000</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.081600</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.120200</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.078800</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.084700</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.089500</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.083700</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.113600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a04ea16dc6141f3844e1f0fd7596b58","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31a8480a387d49dd8ef97a3e516228de","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/13133 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [175/175 05:11, Epoch 50/59]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.005100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>6.882600</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>5.343000</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5.914900</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>5.480600</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>5.832700</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>4.194800</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>3.903300</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.301300</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>3.477600</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>4.417400</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.962500</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>2.653000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>3.018300</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.627900</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.463200</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>2.897600</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.054100</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>2.590500</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.881300</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.798100</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.802000</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.546100</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.543600</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.611200</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.589100</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.095400</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.032800</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.975200</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.105900</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.908200</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.887200</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.759700</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.622600</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.561200</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.607700</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.492000</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.439300</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.362100</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.324000</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.352100</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.422000</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.281900</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.291700</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.416300</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.250800</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.302200</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.310300</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.206200</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.227200</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.229800</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.286800</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.282300</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.226400</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.203500</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.249100</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.279800</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.177800</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.206100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.182600</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.181700</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.189300</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.271500</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.174100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.186800</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.190900</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.266800</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.148300</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.182500</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.212100</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.154100</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.185800</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.216200</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.152600</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.148300</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.214100</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.179300</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.192500</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.211300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.125900</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.153400</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.197000</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.132000</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.157700</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.115700</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.168000</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.176900</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.136700</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.144800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.162900</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.170300</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.148700</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.169400</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.139500</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.106700</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.177900</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.145000</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.111400</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.108500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.121600</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.153000</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.185200</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.116900</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.110700</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.130800</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.170300</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>0.115700</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>0.102700</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.102200</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.103200</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.138600</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>0.126100</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>0.111800</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.137100</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.118700</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.125100</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.109100</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.082000</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.115600</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.106200</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.120900</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.104200</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.117500</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.129400</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.094500</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.079500</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.100500</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.107300</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.083900</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.101000</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.100400</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.095600</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.126200</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.077900</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.119800</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.106300</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.088600</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.078100</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.138600</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.078500</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.083000</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>0.087700</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.126600</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.098900</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.081500</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.097200</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.092600</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.110400</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.093200</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.086000</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.100100</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.076200</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.105800</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.081700</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.105700</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.075300</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.112300</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.071100</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.082800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.077100</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.118500</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.079300</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.110700</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.071800</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.094100</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.073500</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.093300</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.113300</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.079100</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.116200</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.076500</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.082100</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.086900</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.081100</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.110000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c109b41a183344369ae9a63bcc4662c4","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e02ae3b09064d448eb3cf8ae33856a5","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/13133 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:290: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [175/175 05:10, Epoch 50/59]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>5.005100</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>6.882600</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>5.342900</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5.914600</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>5.480400</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>5.832700</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>4.194600</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>3.902800</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>3.301500</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>3.477800</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>4.417900</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>2.962300</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>2.653000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>3.018100</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>2.627400</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>2.462600</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>2.897000</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>2.053600</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>2.590200</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.881300</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.797900</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.802100</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.545800</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.543500</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>1.611400</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>1.588800</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>1.095100</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>1.032700</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.975000</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>1.106100</td>\n","    </tr>\n","    <tr>\n","      <td>31</td>\n","      <td>0.908100</td>\n","    </tr>\n","    <tr>\n","      <td>32</td>\n","      <td>0.887200</td>\n","    </tr>\n","    <tr>\n","      <td>33</td>\n","      <td>0.759400</td>\n","    </tr>\n","    <tr>\n","      <td>34</td>\n","      <td>0.622900</td>\n","    </tr>\n","    <tr>\n","      <td>35</td>\n","      <td>0.561000</td>\n","    </tr>\n","    <tr>\n","      <td>36</td>\n","      <td>0.607600</td>\n","    </tr>\n","    <tr>\n","      <td>37</td>\n","      <td>0.492000</td>\n","    </tr>\n","    <tr>\n","      <td>38</td>\n","      <td>0.439300</td>\n","    </tr>\n","    <tr>\n","      <td>39</td>\n","      <td>0.362000</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.324000</td>\n","    </tr>\n","    <tr>\n","      <td>41</td>\n","      <td>0.352100</td>\n","    </tr>\n","    <tr>\n","      <td>42</td>\n","      <td>0.421900</td>\n","    </tr>\n","    <tr>\n","      <td>43</td>\n","      <td>0.281800</td>\n","    </tr>\n","    <tr>\n","      <td>44</td>\n","      <td>0.291700</td>\n","    </tr>\n","    <tr>\n","      <td>45</td>\n","      <td>0.416100</td>\n","    </tr>\n","    <tr>\n","      <td>46</td>\n","      <td>0.250800</td>\n","    </tr>\n","    <tr>\n","      <td>47</td>\n","      <td>0.302000</td>\n","    </tr>\n","    <tr>\n","      <td>48</td>\n","      <td>0.310200</td>\n","    </tr>\n","    <tr>\n","      <td>49</td>\n","      <td>0.206100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.227100</td>\n","    </tr>\n","    <tr>\n","      <td>51</td>\n","      <td>0.229700</td>\n","    </tr>\n","    <tr>\n","      <td>52</td>\n","      <td>0.286500</td>\n","    </tr>\n","    <tr>\n","      <td>53</td>\n","      <td>0.282300</td>\n","    </tr>\n","    <tr>\n","      <td>54</td>\n","      <td>0.226300</td>\n","    </tr>\n","    <tr>\n","      <td>55</td>\n","      <td>0.203100</td>\n","    </tr>\n","    <tr>\n","      <td>56</td>\n","      <td>0.249100</td>\n","    </tr>\n","    <tr>\n","      <td>57</td>\n","      <td>0.279700</td>\n","    </tr>\n","    <tr>\n","      <td>58</td>\n","      <td>0.177700</td>\n","    </tr>\n","    <tr>\n","      <td>59</td>\n","      <td>0.206100</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.182600</td>\n","    </tr>\n","    <tr>\n","      <td>61</td>\n","      <td>0.181500</td>\n","    </tr>\n","    <tr>\n","      <td>62</td>\n","      <td>0.189200</td>\n","    </tr>\n","    <tr>\n","      <td>63</td>\n","      <td>0.271400</td>\n","    </tr>\n","    <tr>\n","      <td>64</td>\n","      <td>0.174100</td>\n","    </tr>\n","    <tr>\n","      <td>65</td>\n","      <td>0.186800</td>\n","    </tr>\n","    <tr>\n","      <td>66</td>\n","      <td>0.190800</td>\n","    </tr>\n","    <tr>\n","      <td>67</td>\n","      <td>0.266900</td>\n","    </tr>\n","    <tr>\n","      <td>68</td>\n","      <td>0.148300</td>\n","    </tr>\n","    <tr>\n","      <td>69</td>\n","      <td>0.182600</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.212100</td>\n","    </tr>\n","    <tr>\n","      <td>71</td>\n","      <td>0.154100</td>\n","    </tr>\n","    <tr>\n","      <td>72</td>\n","      <td>0.185900</td>\n","    </tr>\n","    <tr>\n","      <td>73</td>\n","      <td>0.216100</td>\n","    </tr>\n","    <tr>\n","      <td>74</td>\n","      <td>0.152700</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.148300</td>\n","    </tr>\n","    <tr>\n","      <td>76</td>\n","      <td>0.214100</td>\n","    </tr>\n","    <tr>\n","      <td>77</td>\n","      <td>0.179300</td>\n","    </tr>\n","    <tr>\n","      <td>78</td>\n","      <td>0.192500</td>\n","    </tr>\n","    <tr>\n","      <td>79</td>\n","      <td>0.211300</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.126000</td>\n","    </tr>\n","    <tr>\n","      <td>81</td>\n","      <td>0.153500</td>\n","    </tr>\n","    <tr>\n","      <td>82</td>\n","      <td>0.197000</td>\n","    </tr>\n","    <tr>\n","      <td>83</td>\n","      <td>0.132200</td>\n","    </tr>\n","    <tr>\n","      <td>84</td>\n","      <td>0.157700</td>\n","    </tr>\n","    <tr>\n","      <td>85</td>\n","      <td>0.115800</td>\n","    </tr>\n","    <tr>\n","      <td>86</td>\n","      <td>0.167900</td>\n","    </tr>\n","    <tr>\n","      <td>87</td>\n","      <td>0.177200</td>\n","    </tr>\n","    <tr>\n","      <td>88</td>\n","      <td>0.136700</td>\n","    </tr>\n","    <tr>\n","      <td>89</td>\n","      <td>0.144800</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.162900</td>\n","    </tr>\n","    <tr>\n","      <td>91</td>\n","      <td>0.170400</td>\n","    </tr>\n","    <tr>\n","      <td>92</td>\n","      <td>0.148700</td>\n","    </tr>\n","    <tr>\n","      <td>93</td>\n","      <td>0.169600</td>\n","    </tr>\n","    <tr>\n","      <td>94</td>\n","      <td>0.139500</td>\n","    </tr>\n","    <tr>\n","      <td>95</td>\n","      <td>0.106500</td>\n","    </tr>\n","    <tr>\n","      <td>96</td>\n","      <td>0.178000</td>\n","    </tr>\n","    <tr>\n","      <td>97</td>\n","      <td>0.145200</td>\n","    </tr>\n","    <tr>\n","      <td>98</td>\n","      <td>0.111200</td>\n","    </tr>\n","    <tr>\n","      <td>99</td>\n","      <td>0.108400</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.121700</td>\n","    </tr>\n","    <tr>\n","      <td>101</td>\n","      <td>0.153100</td>\n","    </tr>\n","    <tr>\n","      <td>102</td>\n","      <td>0.185300</td>\n","    </tr>\n","    <tr>\n","      <td>103</td>\n","      <td>0.117000</td>\n","    </tr>\n","    <tr>\n","      <td>104</td>\n","      <td>0.110800</td>\n","    </tr>\n","    <tr>\n","      <td>105</td>\n","      <td>0.130800</td>\n","    </tr>\n","    <tr>\n","      <td>106</td>\n","      <td>0.170500</td>\n","    </tr>\n","    <tr>\n","      <td>107</td>\n","      <td>0.115700</td>\n","    </tr>\n","    <tr>\n","      <td>108</td>\n","      <td>0.102800</td>\n","    </tr>\n","    <tr>\n","      <td>109</td>\n","      <td>0.102300</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.103200</td>\n","    </tr>\n","    <tr>\n","      <td>111</td>\n","      <td>0.138700</td>\n","    </tr>\n","    <tr>\n","      <td>112</td>\n","      <td>0.126200</td>\n","    </tr>\n","    <tr>\n","      <td>113</td>\n","      <td>0.111800</td>\n","    </tr>\n","    <tr>\n","      <td>114</td>\n","      <td>0.137200</td>\n","    </tr>\n","    <tr>\n","      <td>115</td>\n","      <td>0.118800</td>\n","    </tr>\n","    <tr>\n","      <td>116</td>\n","      <td>0.125200</td>\n","    </tr>\n","    <tr>\n","      <td>117</td>\n","      <td>0.109100</td>\n","    </tr>\n","    <tr>\n","      <td>118</td>\n","      <td>0.082000</td>\n","    </tr>\n","    <tr>\n","      <td>119</td>\n","      <td>0.115800</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.106300</td>\n","    </tr>\n","    <tr>\n","      <td>121</td>\n","      <td>0.121000</td>\n","    </tr>\n","    <tr>\n","      <td>122</td>\n","      <td>0.104200</td>\n","    </tr>\n","    <tr>\n","      <td>123</td>\n","      <td>0.117600</td>\n","    </tr>\n","    <tr>\n","      <td>124</td>\n","      <td>0.129500</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.094600</td>\n","    </tr>\n","    <tr>\n","      <td>126</td>\n","      <td>0.079600</td>\n","    </tr>\n","    <tr>\n","      <td>127</td>\n","      <td>0.100700</td>\n","    </tr>\n","    <tr>\n","      <td>128</td>\n","      <td>0.107300</td>\n","    </tr>\n","    <tr>\n","      <td>129</td>\n","      <td>0.084000</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.101100</td>\n","    </tr>\n","    <tr>\n","      <td>131</td>\n","      <td>0.100500</td>\n","    </tr>\n","    <tr>\n","      <td>132</td>\n","      <td>0.095600</td>\n","    </tr>\n","    <tr>\n","      <td>133</td>\n","      <td>0.126300</td>\n","    </tr>\n","    <tr>\n","      <td>134</td>\n","      <td>0.077900</td>\n","    </tr>\n","    <tr>\n","      <td>135</td>\n","      <td>0.119900</td>\n","    </tr>\n","    <tr>\n","      <td>136</td>\n","      <td>0.106400</td>\n","    </tr>\n","    <tr>\n","      <td>137</td>\n","      <td>0.088600</td>\n","    </tr>\n","    <tr>\n","      <td>138</td>\n","      <td>0.078200</td>\n","    </tr>\n","    <tr>\n","      <td>139</td>\n","      <td>0.138800</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.078500</td>\n","    </tr>\n","    <tr>\n","      <td>141</td>\n","      <td>0.083000</td>\n","    </tr>\n","    <tr>\n","      <td>142</td>\n","      <td>0.087800</td>\n","    </tr>\n","    <tr>\n","      <td>143</td>\n","      <td>0.126800</td>\n","    </tr>\n","    <tr>\n","      <td>144</td>\n","      <td>0.099000</td>\n","    </tr>\n","    <tr>\n","      <td>145</td>\n","      <td>0.081600</td>\n","    </tr>\n","    <tr>\n","      <td>146</td>\n","      <td>0.097200</td>\n","    </tr>\n","    <tr>\n","      <td>147</td>\n","      <td>0.092700</td>\n","    </tr>\n","    <tr>\n","      <td>148</td>\n","      <td>0.110400</td>\n","    </tr>\n","    <tr>\n","      <td>149</td>\n","      <td>0.093300</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.086100</td>\n","    </tr>\n","    <tr>\n","      <td>151</td>\n","      <td>0.100100</td>\n","    </tr>\n","    <tr>\n","      <td>152</td>\n","      <td>0.076200</td>\n","    </tr>\n","    <tr>\n","      <td>153</td>\n","      <td>0.105800</td>\n","    </tr>\n","    <tr>\n","      <td>154</td>\n","      <td>0.081800</td>\n","    </tr>\n","    <tr>\n","      <td>155</td>\n","      <td>0.105800</td>\n","    </tr>\n","    <tr>\n","      <td>156</td>\n","      <td>0.075400</td>\n","    </tr>\n","    <tr>\n","      <td>157</td>\n","      <td>0.112400</td>\n","    </tr>\n","    <tr>\n","      <td>158</td>\n","      <td>0.071100</td>\n","    </tr>\n","    <tr>\n","      <td>159</td>\n","      <td>0.082800</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.077100</td>\n","    </tr>\n","    <tr>\n","      <td>161</td>\n","      <td>0.118600</td>\n","    </tr>\n","    <tr>\n","      <td>162</td>\n","      <td>0.079300</td>\n","    </tr>\n","    <tr>\n","      <td>163</td>\n","      <td>0.110900</td>\n","    </tr>\n","    <tr>\n","      <td>164</td>\n","      <td>0.071800</td>\n","    </tr>\n","    <tr>\n","      <td>165</td>\n","      <td>0.094100</td>\n","    </tr>\n","    <tr>\n","      <td>166</td>\n","      <td>0.073600</td>\n","    </tr>\n","    <tr>\n","      <td>167</td>\n","      <td>0.093300</td>\n","    </tr>\n","    <tr>\n","      <td>168</td>\n","      <td>0.113400</td>\n","    </tr>\n","    <tr>\n","      <td>169</td>\n","      <td>0.079100</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.116300</td>\n","    </tr>\n","    <tr>\n","      <td>171</td>\n","      <td>0.076500</td>\n","    </tr>\n","    <tr>\n","      <td>172</td>\n","      <td>0.082200</td>\n","    </tr>\n","    <tr>\n","      <td>173</td>\n","      <td>0.086900</td>\n","    </tr>\n","    <tr>\n","      <td>174</td>\n","      <td>0.081200</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.110100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Best model type: adamw_hf\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from datasets import Dataset, DatasetDict\n","from trl import SFTTrainer\n","import transformers\n","import json\n","import pandas as pd\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from datasets import Dataset, DatasetDict\n","from trl import SFTTrainer\n","import transformers\n","import json\n","import csv\n","\n","def load_custom_jsonl_to_dataset(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        data = [json.loads(line.strip()) for line in file]\n","    dataset = Dataset.from_dict({\"question\": [item[\"question\"] for item in data],\n","                                 \"answer\": [item[\"answer\"] for item in data]})\n","    return DatasetDict({\"train\": dataset})\n","\n","def formatting_func(example):\n","    text = f\"<start_of_turn>user\\n{example['question'][0]}<end_of_turn> <start_of_turn>model\\n{example['answer'][0]}<end_of_turn>\"\n","    return [text]\n","\n","jsonl_file_path = \"/kaggle/input/complete/CompleteDataset.jsonl\"\n","custom_dataset = load_custom_jsonl_to_dataset(jsonl_file_path)\n","\n","# List of optimization parameters\n","optim_params = [\n","    {\"optim\": \"adamw_hf\"},\n","    {\"optim\": \"adamw_torch\"},\n","    {\"optim\": \"adamw_torch_fused\"},\n","    # Add more optimization parameters as needed\n","]\n","\n","# Initialize best loss and model type\n","best_loss = float('inf')\n","best_model_type = None\n","\n","# Create a CSV file for logging\n","with open('training_logs.csv', 'w', newline='') as csvfile:\n","    csv_writer = csv.writer(csvfile)\n","    csv_writer.writerow([\"model_type\", \"step\", \"training_loss\"])\n","    \n","    # Iterate over each optimization parameter set\n","    for params in optim_params:\n","        model_id = \"google/gemma-2b-it\"\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=\"nf4\",\n","            bnb_4bit_compute_dtype=torch.bfloat16\n","        )\n","\n","        tokenizer = AutoTokenizer.from_pretrained(model_id, token=secret_value_0)\n","        model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=secret_value_0)\n","\n","        # Instantiate the SFTTrainer with the current set of parameters\n","        trainer = SFTTrainer(\n","            model=model,\n","            train_dataset=custom_dataset[\"train\"],\n","            args=transformers.TrainingArguments(\n","                per_device_train_batch_size=1,\n","                gradient_accumulation_steps=4,\n","                warmup_steps=5,\n","                max_steps=175,\n","                learning_rate=2e-4,\n","                fp16=True,\n","                logging_steps=1,\n","                output_dir=\"outputs\",\n","                **params  # Include the current optimization parameter set\n","            ),\n","            peft_config=lora_config,\n","            formatting_func=formatting_func,\n","        )\n","\n","        # Train the model\n","        trainer.train()\n","\n","        # Convert training logs to a DataFrame\n","        train_logs = trainer.state.log_history\n","        training_logs_df = pd.DataFrame(train_logs)\n","\n","        # Add model type to the DataFrame\n","        training_logs_df['model_type'] = params[\"optim\"]\n","        \n","        # Save training logs to the CSV file\n","        training_logs_df.to_csv(csvfile, mode='a', header=False, index=False)\n","\n","        # Get the final loss\n","        final_loss = trainer.state.global_step\n","\n","        # Update the best loss and model type if applicable\n","        if final_loss is not None and final_loss < best_loss:\n","            best_loss = final_loss\n","            best_model_type = params[\"optim\"]\n","\n","# Output the best model type\n","print(\"Best model type:\", best_model_type)\n"]},{"cell_type":"markdown","metadata":{},"source":["1bdc021ee45512579f9d8463c99aabb7b0e97590"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:56:12.462669Z","iopub.status.busy":"2024-03-25T16:56:12.461934Z","iopub.status.idle":"2024-03-25T16:56:25.744298Z","shell.execute_reply":"2024-03-25T16:56:25.743020Z","shell.execute_reply.started":"2024-03-25T16:56:12.462626Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.21.4)\n","Collecting huggingface_hub\n","  Downloading huggingface_hub-0.22.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n","Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","Downloading huggingface_hub-0.22.0-py3-none-any.whl (388 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.5/388.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface_hub\n","  Attempting uninstall: huggingface_hub\n","    Found existing installation: huggingface-hub 0.21.4\n","    Uninstalling huggingface-hub-0.21.4:\n","      Successfully uninstalled huggingface-hub-0.21.4\n","Successfully installed huggingface_hub-0.22.0\n"]}],"source":["!pip install --upgrade huggingface_hub\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:56:25.746037Z","iopub.status.busy":"2024-03-25T16:56:25.745703Z","iopub.status.idle":"2024-03-25T16:56:25.903472Z","shell.execute_reply":"2024-03-25T16:56:25.902081Z","shell.execute_reply.started":"2024-03-25T16:56:25.746009Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n","Token is valid (permission: write).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]}],"source":["from huggingface_hub import login\n","login(token = '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')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:56:25.905385Z","iopub.status.busy":"2024-03-25T16:56:25.904987Z","iopub.status.idle":"2024-03-25T16:56:26.285173Z","shell.execute_reply":"2024-03-25T16:56:26.284033Z","shell.execute_reply.started":"2024-03-25T16:56:25.905348Z"},"trusted":true},"outputs":[],"source":["# Save trained model\n","trainer.model.save_pretrained(best_model_type)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:56:26.286730Z","iopub.status.busy":"2024-03-25T16:56:26.286435Z","iopub.status.idle":"2024-03-25T16:56:26.292702Z","shell.execute_reply":"2024-03-25T16:56:26.290685Z","shell.execute_reply.started":"2024-03-25T16:56:26.286705Z"},"trusted":true},"outputs":[],"source":["# !huggingface-cli login"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T16:56:26.294705Z","iopub.status.busy":"2024-03-25T16:56:26.294394Z","iopub.status.idle":"2024-03-25T16:58:07.629995Z","shell.execute_reply":"2024-03-25T16:58:07.628780Z","shell.execute_reply.started":"2024-03-25T16:56:26.294681Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d26d8126e978494bb81e010c2de9d4ca","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"16a612f477694b3189c42cb67573e470","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"440044f36cb74f35a2e628b45009a353","version_major":2,"version_minor":0},"text/plain":["Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53734a399994451baf408bb618190878","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a59288980bc2414e919cffcaaf20a003","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/Sahi19/Gemma2bLegalChatbot/commit/e45013e38e4149cfdc04b1c30da1ca2ed25324f4', commit_message='Upload tokenizer', commit_description='', oid='e45013e38e4149cfdc04b1c30da1ca2ed25324f4', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["## push maually\n","model.push_to_hub(\"Sahi19/Gemma2bLegalChatbot\")\n","tokenizer.push_to_hub(\"Sahi19/Gemma2bLegalChatbot\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4662827,"sourceId":7932562,"sourceType":"datasetVersion"},{"datasetId":4662837,"sourceId":7932574,"sourceType":"datasetVersion"},{"datasetId":4662985,"sourceId":7932794,"sourceType":"datasetVersion"},{"datasetId":4662997,"sourceId":7932813,"sourceType":"datasetVersion"}],"dockerImageVersionId":30674,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
